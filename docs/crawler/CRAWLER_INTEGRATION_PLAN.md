# Plano de IntegraÃ§Ã£o: Web Crawler Agendado
**Sistema Ingestify - MÃ³dulo Crawler**

**VersÃ£o:** 1.2 (Atualizado)
**Data:** 2025-01-13
**Status:** Planejamento - Atualizado com retry inteligente e fallback de engines
**Changelog:** Adicionado sistema de retry com fallback progressivo de engines

---

## 1. VisÃ£o Geral

### 1.1 Objetivo

Integrar funcionalidade completa de **web crawler agendado** ao sistema Ingestify, permitindo:

- âœ… Agendamento de crawls com frequÃªncia flexÃ­vel (one-time, hourly, daily, weekly, monthly, custom cron)
- âœ… **MÃºltiplas engines de crawling:**
  - BeautifulSoup (bs4) - rÃ¡pido, leve, para sites estÃ¡ticos
  - BeautifulSoup + Proxy - para sites com restriÃ§Ã£o geogrÃ¡fica
  - Playwright - para sites com JavaScript/SPA
  - Playwright + Proxy - para sites JS com restriÃ§Ã£o geogrÃ¡fica
- âœ… **Download flexÃ­vel de assets:**
  - HTML only (sem assets)
  - HTML + Assets selecionados (CSS, JS, Images, Fonts, Videos, Documents)
- âœ… Download de pÃ¡ginas HTML + arquivos especÃ­ficos (PDF, XLSX, CSV, imagens, etc.)
- âœ… Merge de PDFs (individual, combinado ou ambos)
- âœ… DetecÃ§Ã£o inteligente de duplicatas via padrÃ£o de URL
- âœ… Rastreamento de progresso em tempo real
- âœ… HistÃ³rico completo de execuÃ§Ãµes com mÃ©tricas
- âœ… Armazenamento em Min.io com URLs pÃºblicas
- âœ… Busca e analytics via Elasticsearch

---

### 1.2 Features Principais (Novo na v1.1)

#### ğŸ”§ MÃºltiplas Engines de Crawling

| Engine | DescriÃ§Ã£o | Use Case |
|--------|-----------|----------|
| **BeautifulSoup** | RÃ¡pido, leve, para HTML estÃ¡tico | Blogs, documentaÃ§Ã£o, sites simples |
| **BeautifulSoup + Proxy** | BeautifulSoup com proxy | Sites estÃ¡ticos com geo-blocking |
| **Playwright** | JavaScript rendering, SPAs | React, Vue, Angular, dashboards |
| **Playwright + Proxy** | Playwright com proxy | Sites JS com geo-blocking |

#### ğŸ“¦ Download Granular de Assets

Controle preciso sobre o que baixar alÃ©m do HTML:

- **HTML Only** - Apenas o cÃ³digo HTML (rÃ¡pido, leve)
- **HTML + CSS** - HTML + estilos (visualizaÃ§Ã£o bÃ¡sica)
- **HTML + CSS + Images** - PÃ¡gina completa sem interatividade
- **HTML + CSS + JS + Images** - PÃ¡gina funcional offline
- **Full (CSS + JS + Images + Fonts + Videos)** - Arquivamento completo

**Asset Types disponÃ­veis:**
- `css` - Arquivos .css
- `js` - Arquivos .js
- `images` - .jpg, .png, .gif, .svg, .webp, .ico
- `fonts` - .woff, .woff2, .ttf, .otf
- `videos` - .mp4, .webm, .ogg
- `documents` - .pdf, .docx, .xlsx (linkados na pÃ¡gina)

#### ğŸŒ Suporte a Proxies

- **Protocolos:** HTTP, HTTPS, SOCKS5
- **AutenticaÃ§Ã£o:** Username/password
- **ConfiguraÃ§Ã£o por job** (cada crawler pode ter seu proxy)
- **RotaÃ§Ã£o de proxies** (future: pool de proxies)

#### ğŸ”„ Sistema de Retry Inteligente (Novo na v1.2)

**Fallback progressivo de engines** - Aumenta a "potÃªncia" a cada retry atÃ© obter sucesso:

```
Tentativa 0: BeautifulSoup (rÃ¡pido, leve)
     â†“ FALHOU (timeout)
Retry 1: BeautifulSoup + Proxy (bypassa bloqueio)
     â†“ FALHOU (403)
Retry 2: Playwright (renderiza JavaScript)
     â†“ FALHOU (erro JS)
Retry 3: Playwright + Proxy (mÃ¡xima compatibilidade)
     â†“ SUCESSO âœ…
```

**EstratÃ©gias prÃ©-definidas:**
- **Conservative** - BS4 â†’ BS4+Proxy â†’ Playwright â†’ Playwright+Proxy
- **Aggressive** - Playwright â†’ Playwright+Proxy (para sites JS conhecidos)
- **Proxy First** - Sempre com proxy (geo-blocking conhecido)
- **Balanced** - Mix de todas as combinaÃ§Ãµes

**Features:**
- âœ… Backoff exponencial (delays crescentes)
- âœ… HistÃ³rico completo de tentativas (`retry_history`)
- âœ… MÃ©tricas de retry em Elasticsearch
- âœ… ConfigurÃ¡vel por crawler

---

### 1.3 Infraestrutura Existente (Reuso 100%)

O sistema Ingestify **jÃ¡ possui toda a infraestrutura necessÃ¡ria**:

| Componente | Uso no Crawler |
|------------|----------------|
| **Celery + Celery Beat** | Agendamento e execuÃ§Ã£o de crawls |
| **MySQL** | PersistÃªncia de configuraÃ§Ãµes e histÃ³rico |
| **Elasticsearch** | Metadados, busca fuzzy, mÃ©tricas time-series |
| **Min.io** | Armazenamento de arquivos crawleados |
| **Redis** | Filas Celery e cache de progresso |
| **Clean Architecture** | Estrutura modular para novos mÃ³dulos |
| **Auth (JWT + API Keys)** | SeguranÃ§a e controle de acesso |

**ConclusÃ£o:** Zero infraestrutura nova. Apenas novos mÃ³dulos integrados.

---

## 2. Arquitetura de Alto NÃ­vel

### 2.1 Camadas da Clean Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PRESENTATION LAYER                      â”‚
â”‚  API Controllers: /crawlers/* (REST endpoints)           â”‚
â”‚  Schemas: Requests e Responses (Pydantic)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  APPLICATION LAYER                       â”‚
â”‚  Use Cases:                                              â”‚
â”‚   - CreateCrawlerJobUseCase                              â”‚
â”‚   - ExecuteCrawlerJobUseCase                             â”‚
â”‚   - ListCrawlerJobsUseCase                               â”‚
â”‚   - UpdateCrawlerJobUseCase                              â”‚
â”‚   - GetCrawlerExecutionHistoryUseCase                    â”‚
â”‚   - PauseCrawlerJobUseCase                               â”‚
â”‚  DTOs: CrawlerJobDTO, CrawlerExecutionDTO                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOMAIN LAYER                          â”‚
â”‚  Entities:                                               â”‚
â”‚   - CrawlerJob (configuraÃ§Ã£o do crawler)                 â”‚
â”‚   - CrawlerExecution (execuÃ§Ã£o individual)               â”‚
â”‚   - CrawledFile (arquivo baixado)                        â”‚
â”‚  Value Objects:                                          â”‚
â”‚   - URLPattern (normalizaÃ§Ã£o para duplicatas)            â”‚
â”‚   - CrawlerSchedule (configuraÃ§Ã£o de agendamento)        â”‚
â”‚   - CrawlerEngine (bs4/playwright + proxy)              â”‚
â”‚   - ProxyConfig (host, port, auth, protocol)            â”‚
â”‚   - AssetTypes (tipos de assets a baixar)               â”‚
â”‚   - DownloadConfig (filtros de arquivo, PDFs)           â”‚
â”‚  Services:                                               â”‚
â”‚   - URLNormalizerService (normalizar URLs)               â”‚
â”‚   - DuplicateDetectorService (detectar duplicatas)       â”‚
â”‚   - CrawlerProgressService (calcular progresso)          â”‚
â”‚  Repositories (interfaces):                              â”‚
â”‚   - CrawlerJobRepository                                 â”‚
â”‚   - CrawlerExecutionRepository                           â”‚
â”‚   - CrawledFileRepository                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                INFRASTRUCTURE LAYER                      â”‚
â”‚  Repositories (MySQL):                                   â”‚
â”‚   - MySQLCrawlerJobRepository                            â”‚
â”‚   - MySQLCrawlerExecutionRepository                      â”‚
â”‚   - MySQLCrawledFileRepository                           â”‚
â”‚  Adapters:                                               â”‚
â”‚   - BeautifulSoupCrawlerAdapter (scraping estÃ¡tico)      â”‚
â”‚   - PlaywrightCrawlerAdapter (scraping JS/SPA)          â”‚
â”‚   - ProxyManager (gestÃ£o de proxies)                    â”‚
â”‚   - PyPDFMergerAdapter (merge de PDFs)                   â”‚
â”‚   - MinioCrawlerStorageAdapter (storage)                 â”‚
â”‚  Elasticsearch:                                          â”‚
â”‚   - CrawlerJobIndex                                      â”‚
â”‚   - CrawlerExecutionIndex (time-series)                  â”‚
â”‚   - CrawlerMetricsIndex (mÃ©tricas tempo real)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      WORKERS LAYER                       â”‚
â”‚  Celery Tasks:                                           â”‚
â”‚   - execute_crawler (task principal)                     â”‚
â”‚   - schedule_crawler (Celery Beat trigger)               â”‚
â”‚  Worker Logic:                                           â”‚
â”‚   - CrawlerScraper (orquestraÃ§Ã£o do crawl)               â”‚
â”‚   - RetryManager (gerenciamento de retries)              â”‚
â”‚   - FileDownloader (download paralelo)                   â”‚
â”‚   - PDFProcessor (merge de PDFs)                         â”‚
â”‚   - ProgressTracker (atualizaÃ§Ã£o tempo real)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Fluxo de Dados

```
USER â†’ API â†’ Use Case â†’ Domain â†’ Infrastructure â†’ Workers
                                       â†“
                                MySQL + Elasticsearch + Min.io
                                       â†“
                                Celery Beat (agendamento)
                                       â†“
                                Celery Workers (execuÃ§Ã£o)
```

---

## 3. Modelos de Dados

### 3.1 MySQL (PersistÃªncia)

#### Tabela: `crawler_jobs`
**ConfiguraÃ§Ã£o de crawler agendado**

| Campo | Tipo | DescriÃ§Ã£o |
|-------|------|-----------|
| `id` | VARCHAR(36) PK | UUID do crawler |
| `user_id` | VARCHAR(36) FK | Dono do crawler |
| `name` | VARCHAR(255) | Nome amigÃ¡vel |
| `url` | TEXT | URL base para crawl |
| `url_pattern` | TEXT | PadrÃ£o normalizado (detecÃ§Ã£o duplicatas) |
| `crawler_engine` | ENUM | BEAUTIFULSOUP / PLAYWRIGHT |
| `use_proxy` | BOOLEAN | Usar proxy |
| `proxy_config` | JSON | {"host": "...", "port": 8080, "username": "...", "password": "...", "protocol": "http"} |
| `crawl_type` | ENUM | PAGE_ONLY / PAGE_WITH_ALL / PAGE_WITH_FILTERED / FULL_WEBSITE |
| `max_depth` | INTEGER | Profundidade de crawl (para FULL_WEBSITE) |
| `follow_external_links` | BOOLEAN | Seguir links externos |
| `download_assets` | BOOLEAN | Baixar assets (CSS, JS, images, etc.) |
| `asset_types` | JSON | ["css", "js", "images", "fonts", "videos"] ou [] para HTML only |
| `file_extensions` | JSON | ["pdf", "xlsx", "csv"] - arquivos para download |
| `extension_categories` | JSON | ["documents", "images"] |
| `pdf_handling` | ENUM | INDIVIDUAL / COMBINED / BOTH |
| `retry_enabled` | BOOLEAN | Habilitar retries em caso de erro |
| `max_retries` | INTEGER | NÃºmero mÃ¡ximo de retries (default: 3) |
| `retry_strategy` | JSON | EstratÃ©gia de retry (fallback de engines) |
| `schedule_type` | ENUM | ONE_TIME / RECURRING |
| `schedule_frequency` | ENUM | HOURLY / DAILY / WEEKLY / MONTHLY / CUSTOM |
| `cron_expression` | VARCHAR(100) | ExpressÃ£o cron (para CUSTOM) |
| `timezone` | VARCHAR(50) | Timezone (default: UTC) |
| `next_run_at` | DATETIME | PrÃ³xima execuÃ§Ã£o |
| `is_active` | BOOLEAN | Ativo ou pausado |
| `status` | ENUM | ACTIVE / PAUSED / STOPPED / ERROR |
| `total_executions` | INTEGER | Total de execuÃ§Ãµes |
| `successful_executions` | INTEGER | ExecuÃ§Ãµes bem-sucedidas |
| `failed_executions` | INTEGER | ExecuÃ§Ãµes com falha |
| `last_execution_at` | DATETIME | Ãšltima execuÃ§Ã£o |
| `created_at` | DATETIME | CriaÃ§Ã£o |
| `updated_at` | DATETIME | Ãšltima atualizaÃ§Ã£o |

**Relacionamentos:**
- `user_id` â†’ `users.id` (FK)
- `crawler_jobs.id` â† `crawler_executions.crawler_job_id` (1:N)

---

#### Tabela: `crawler_executions`
**HistÃ³rico de execuÃ§Ã£o de um crawler**

| Campo | Tipo | DescriÃ§Ã£o |
|-------|------|-----------|
| `id` | VARCHAR(36) PK | UUID da execuÃ§Ã£o |
| `crawler_job_id` | VARCHAR(36) FK | Crawler que originou |
| `celery_task_id` | VARCHAR(36) | Task ID do Celery |
| `status` | ENUM | PENDING / PROCESSING / COMPLETED / FAILED / CANCELLED |
| `progress` | INTEGER | 0-100% |
| `pages_discovered` | INTEGER | PÃ¡ginas descobertas |
| `pages_downloaded` | INTEGER | PÃ¡ginas baixadas |
| `pages_failed` | INTEGER | PÃ¡ginas com erro |
| `files_downloaded` | INTEGER | Arquivos baixados |
| `files_failed` | INTEGER | Arquivos com erro |
| `total_size_bytes` | INTEGER | Tamanho total (bytes) |
| `files_by_type` | JSON | {"pdf": 10, "xlsx": 5} |
| `minio_folder_path` | VARCHAR(500) | Pasta no Min.io |
| `error_message` | TEXT | Mensagem de erro |
| `error_count` | INTEGER | Quantidade de erros |
| `retry_count` | INTEGER | NÃºmero de retries executados (default: 0) |
| `current_retry_attempt` | INTEGER | Tentativa atual (0 = primeira tentativa) |
| `retry_history` | JSON | HistÃ³rico de retries com engines usadas |
| `engine_used` | ENUM | Engine que finalizou com sucesso (BEAUTIFULSOUP / PLAYWRIGHT) |
| `proxy_used` | BOOLEAN | Se proxy foi usado na tentativa final |
| `started_at` | DATETIME | InÃ­cio |
| `completed_at` | DATETIME | Fim |
| `created_at` | DATETIME | CriaÃ§Ã£o |
| `updated_at` | DATETIME | Ãšltima atualizaÃ§Ã£o |

**Relacionamentos:**
- `crawler_job_id` â†’ `crawler_jobs.id` (FK)
- `crawler_executions.id` â† `crawled_files.execution_id` (1:N)

---

#### Tabela: `crawled_files`
**Arquivo individual baixado durante uma execuÃ§Ã£o**

| Campo | Tipo | DescriÃ§Ã£o |
|-------|------|-----------|
| `id` | VARCHAR(36) PK | UUID do arquivo |
| `execution_id` | VARCHAR(36) FK | ExecuÃ§Ã£o que baixou |
| `url` | TEXT | URL original |
| `filename` | VARCHAR(255) | Nome do arquivo |
| `file_type` | VARCHAR(20) | pdf, xlsx, jpg, etc. |
| `mime_type` | VARCHAR(100) | Content-Type |
| `size_bytes` | INTEGER | Tamanho |
| `minio_path` | VARCHAR(500) | Path no Min.io |
| `minio_bucket` | VARCHAR(100) | Bucket do Min.io |
| `public_url` | TEXT | URL pÃºblica do Min.io |
| `status` | ENUM | DOWNLOADED / FAILED / SKIPPED |
| `error_message` | TEXT | Erro (se houver) |
| `downloaded_at` | DATETIME | Data do download |

**Relacionamentos:**
- `execution_id` â†’ `crawler_executions.id` (FK)

---

### 3.2 Elasticsearch (Busca + Analytics)

#### Index: `crawler-jobs-*`
**Jobs de crawler indexados para busca**

**PropÃ³sito:** Busca fuzzy de URLs, filtros, agregaÃ§Ãµes

**Campos principais:**
- `job_id` (keyword)
- `user_id` (keyword)
- `name` (text + keyword)
- `url` (keyword)
- `url_pattern` (text com analyzer 'standard' para fuzzy match)
- `crawl_type` (keyword)
- `schedule_type`, `schedule_frequency` (keyword)
- `is_active`, `status` (keyword)
- `total_executions`, `successful_executions`, `failed_executions` (integer)
- `last_execution_at`, `next_run_at`, `created_at`, `updated_at` (date)
- `tags` (keyword multi-valued para filtros)

**Uso:**
- Buscar crawlers por URL similar (fuzzy matching)
- Detectar duplicatas via `url_pattern`
- Filtrar por status, tipo, usuÃ¡rio
- AgregaÃ§Ãµes (crawlers por domÃ­nio, por status, etc.)

---

#### Index: `crawler-executions-*` (time-series)
**HistÃ³rico de execuÃ§Ãµes (time-series para analytics)**

**PropÃ³sito:** AnÃ¡lise histÃ³rica, mÃ©tricas, dashboards

**Campos principais:**
- `execution_id` (keyword)
- `crawler_job_id` (keyword)
- `status` (keyword)
- `progress` (integer)
- `pages_discovered`, `pages_downloaded`, `pages_failed` (integer)
- `files_downloaded`, `files_failed`, `total_size_bytes` (integer)
- `files_by_type` (nested)
- `duration_seconds` (integer)
- `average_download_speed_mbps` (float)
- `started_at`, `completed_at` (date)

**Uso:**
- AnÃ¡lise de performance (velocidade, taxa de sucesso)
- Dashboards de mÃ©tricas
- Queries temporais (Ãºltimas 24h, 7d, 30d)
- AgregaÃ§Ãµes (execuÃ§Ãµes por status, por crawler, por perÃ­odo)

---

#### Index: `crawler-metrics-YYYY.MM.DD` (mÃ©tricas tempo real)
**MÃ©tricas em tempo real de execuÃ§Ã£o**

**PropÃ³sito:** Monitoramento ao vivo, troubleshooting

**Campos principais:**
- `execution_id` (keyword)
- `timestamp` (date)
- `progress_percentage` (float)
- `pages_processed`, `files_processed`, `bytes_downloaded` (integer)
- `download_speed_bps`, `response_time_ms` (float)
- `memory_mb`, `cpu_percent` (float)
- `error_count` (integer)
- `errors` (text multi-valued)

**Uso:**
- Rastreamento de progresso em tempo real
- DetecÃ§Ã£o de problemas (lentidÃ£o, erros)
- AnÃ¡lise de performance por execuÃ§Ã£o
- Ãndices com TTL (deletar apÃ³s N dias)

**ConfiguraÃ§Ã£o:**
- `refresh_interval: 5s` (near real-time)
- `number_of_shards: 1` (baixo volume)
- ILM para rollover diÃ¡rio e deleÃ§Ã£o automÃ¡tica

---

### 3.3 Min.io (Storage)

#### Bucket: `ingestify-crawled`

**Estrutura de pastas:**
```
ingestify-crawled/
â”œâ”€â”€ crawled/
â”‚   â”œâ”€â”€ {execution_id_1}/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ example.com_page1.html
â”‚   â”‚   â”‚   â””â”€â”€ example.com_page2.html
â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”‚   â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ app.js
â”‚   â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ logo.png
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ banner.jpg
â”‚   â”‚   â”‚   â”œâ”€â”€ fonts/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ roboto.woff2
â”‚   â”‚   â”‚   â””â”€â”€ videos/
â”‚   â”‚   â”‚       â””â”€â”€ demo.mp4
â”‚   â”‚   â”œâ”€â”€ files/
â”‚   â”‚   â”‚   â”œâ”€â”€ document1.pdf
â”‚   â”‚   â”‚   â”œâ”€â”€ report.xlsx
â”‚   â”‚   â”‚   â””â”€â”€ data.csv
â”‚   â”‚   â””â”€â”€ merged/
â”‚   â”‚       â””â”€â”€ merged_{execution_id_1}.pdf
â”‚   â”œâ”€â”€ {execution_id_2}/
â”‚   â”‚   â”œâ”€â”€ ...
```

**PolÃ­ticas:**
- Public read para facilitar acesso (URLs pÃºblicas)
- Presigned URLs com expiraÃ§Ã£o para seguranÃ§a adicional (opcional)

**URLs pÃºblicas:**
- Formato: `http://minio_host:9000/ingestify-crawled/crawled/{execution_id}/files/{filename}`
- Armazenadas em `crawled_files.public_url`

---

### 3.4 OpÃ§Ãµes de Crawler Engine e Assets

#### 3.4.1 Crawler Engines

**Objetivo:** Permitir escolha da engine de crawling baseada nas necessidades do site.

| Engine | Quando Usar | Vantagens | Desvantagens |
|--------|-------------|-----------|--------------|
| **BeautifulSoup** | Sites estÃ¡ticos, HTML puro | RÃ¡pido, leve, baixo consumo de recursos | NÃ£o executa JavaScript |
| **BeautifulSoup + Proxy** | Sites estÃ¡ticos com restriÃ§Ã£o geogrÃ¡fica | Bypassa bloqueios regionais | Custo de proxy |
| **Playwright** | Sites com JavaScript, SPAs (React, Vue, Angular) | Renderiza JS, suporta interaÃ§Ãµes complexas | Mais lento, maior consumo de recursos |
| **Playwright + Proxy** | Sites JS com restriÃ§Ã£o geogrÃ¡fica | Combina JS rendering + proxy | Mais lento, maior custo |

**ConfiguraÃ§Ã£o:**
```json
{
  "crawler_engine": "BEAUTIFULSOUP",  // ou "PLAYWRIGHT"
  "use_proxy": false,
  "proxy_config": {
    "host": "proxy.example.com",
    "port": 8080,
    "username": "user",
    "password": "pass",
    "protocol": "http"  // ou "https", "socks5"
  }
}
```

**Casos de uso:**
- **Blog estÃ¡tico:** BeautifulSoup
- **Site React/Vue:** Playwright
- **Site bloqueado no BR:** BeautifulSoup + Proxy
- **Dashboard JS bloqueado:** Playwright + Proxy

---

#### 3.4.2 Asset Types (Download de Recursos)

**Objetivo:** Controlar quais recursos da pÃ¡gina sÃ£o baixados alÃ©m do HTML.

**OpÃ§Ãµes:**

| Asset Type | ExtensÃµes | Exemplos | Uso |
|------------|-----------|----------|-----|
| **css** | .css | style.css, bootstrap.css | Estilos da pÃ¡gina |
| **js** | .js | app.js, jquery.min.js | Scripts |
| **images** | .jpg, .jpeg, .png, .gif, .svg, .webp, .ico | logo.png, banner.jpg | Imagens |
| **fonts** | .woff, .woff2, .ttf, .otf, .eot | roboto.woff2 | Fontes customizadas |
| **videos** | .mp4, .webm, .ogg | demo.mp4, tutorial.webm | VÃ­deos embarcados |
| **documents** | .pdf, .docx, .xlsx, .pptx | manual.pdf, report.xlsx | Documentos linkados |

**Modos de configuraÃ§Ã£o:**

**1. HTML Only (sem assets):**
```json
{
  "download_assets": false,
  "asset_types": []
}
```
- Baixa apenas o HTML
- Mais rÃ¡pido
- Menor uso de storage
- Ideal para extraÃ§Ã£o de texto

**2. HTML + Assets Selecionados:**
```json
{
  "download_assets": true,
  "asset_types": ["css", "images"]
}
```
- Baixa HTML + CSS + imagens
- PÃ¡gina pode ser visualizada offline (sem JS)
- Tamanho mÃ©dio

**3. HTML + Todos os Assets:**
```json
{
  "download_assets": true,
  "asset_types": ["css", "js", "images", "fonts", "videos"]
}
```
- Download completo para navegaÃ§Ã£o offline
- Maior tempo de download
- Maior uso de storage

**LÃ³gica de download:**
- Assets sÃ£o detectados via parsing de tags HTML: `<link>`, `<script>`, `<img>`, `<video>`, `@font-face`, etc.
- URLs sÃ£o resolvidas para absolutos
- Downloads em paralelo (max: 10 simultÃ¢neos)
- Assets organizados por tipo em Min.io

**EstatÃ­sticas por tipo:**
```json
{
  "assets_downloaded": {
    "css": 5,
    "js": 12,
    "images": 34,
    "fonts": 3,
    "videos": 1
  },
  "total_assets_size_bytes": 15728640
}
```

---

### 3.5 Sistema de Retry com Fallback de Engines

#### 3.5.1 Objetivo

**Problema:** Sites podem falhar por diversos motivos (timeout, bloqueio, JavaScript nÃ£o carregado, etc.)

**SoluÃ§Ã£o:** Sistema inteligente de retry com **fallback progressivo de engines**, aumentando gradualmente a "potÃªncia" da engine atÃ© obter sucesso.

---

#### 3.5.2 EstratÃ©gia de Retry

**Conceito:** Cada retry usa uma engine diferente e mais robusta que a anterior.

**Exemplo prÃ¡tico:**
```
Tentativa 0 (inicial): BeautifulSoup (rÃ¡pido, leve)
   â†“ FALHOU
Retry 1: BeautifulSoup + Proxy (bypassa geo-blocking)
   â†“ FALHOU
Retry 2: Playwright (renderiza JavaScript)
   â†“ FALHOU
Retry 3: Playwright + Proxy (mÃ¡xima compatibilidade)
   â†“ SUCESSO âœ…
```

---

#### 3.5.3 ConfiguraÃ§Ã£o de Retry Strategy

**Estrutura JSON:**
```json
{
  "retry_enabled": true,
  "max_retries": 3,
  "retry_strategy": [
    {
      "attempt": 0,
      "engine": "BEAUTIFULSOUP",
      "use_proxy": false,
      "delay_seconds": 0
    },
    {
      "attempt": 1,
      "engine": "BEAUTIFULSOUP",
      "use_proxy": true,
      "delay_seconds": 5
    },
    {
      "attempt": 2,
      "engine": "PLAYWRIGHT",
      "use_proxy": false,
      "delay_seconds": 10
    },
    {
      "attempt": 3,
      "engine": "PLAYWRIGHT",
      "use_proxy": true,
      "delay_seconds": 15
    }
  ]
}
```

**Campos:**
- `attempt` - NÃºmero da tentativa (0 = primeira, 1 = retry 1, etc.)
- `engine` - Engine a usar (BEAUTIFULSOUP / PLAYWRIGHT)
- `use_proxy` - Se deve usar proxy nesta tentativa
- `delay_seconds` - Delay antes de executar (backoff exponencial)

---

#### 3.5.4 EstratÃ©gias PrÃ©-definidas (Templates)

**1. Conservative (BeautifulSoup First)**
```json
{
  "name": "conservative",
  "max_retries": 3,
  "strategy": [
    {"attempt": 0, "engine": "BEAUTIFULSOUP", "use_proxy": false, "delay_seconds": 0},
    {"attempt": 1, "engine": "BEAUTIFULSOUP", "use_proxy": true, "delay_seconds": 5},
    {"attempt": 2, "engine": "PLAYWRIGHT", "use_proxy": false, "delay_seconds": 10},
    {"attempt": 3, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 15}
  ]
}
```
**Quando usar:** Sites com alta probabilidade de serem estÃ¡ticos, proxy como fallback

---

**2. Aggressive (Playwright First)**
```json
{
  "name": "aggressive",
  "max_retries": 2,
  "strategy": [
    {"attempt": 0, "engine": "PLAYWRIGHT", "use_proxy": false, "delay_seconds": 0},
    {"attempt": 1, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 10},
    {"attempt": 2, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 20}
  ]
}
```
**Quando usar:** Sites JavaScript conhecidos (SPAs, dashboards)

---

**3. Proxy First**
```json
{
  "name": "proxy_first",
  "max_retries": 3,
  "strategy": [
    {"attempt": 0, "engine": "BEAUTIFULSOUP", "use_proxy": true, "delay_seconds": 0},
    {"attempt": 1, "engine": "BEAUTIFULSOUP", "use_proxy": true, "delay_seconds": 5},
    {"attempt": 2, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 10},
    {"attempt": 3, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 20}
  ]
}
```
**Quando usar:** Sites com geo-blocking conhecido

---

**4. Balanced (Mix)**
```json
{
  "name": "balanced",
  "max_retries": 4,
  "strategy": [
    {"attempt": 0, "engine": "BEAUTIFULSOUP", "use_proxy": false, "delay_seconds": 0},
    {"attempt": 1, "engine": "BEAUTIFULSOUP", "use_proxy": true, "delay_seconds": 3},
    {"attempt": 2, "engine": "PLAYWRIGHT", "use_proxy": false, "delay_seconds": 8},
    {"attempt": 3, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 15},
    {"attempt": 4, "engine": "PLAYWRIGHT", "use_proxy": true, "delay_seconds": 30}
  ]
}
```
**Quando usar:** Sites desconhecidos, mÃ¡xima cobertura

---

#### 3.5.5 HistÃ³rico de Retry

**Rastreamento de tentativas:**

Cada tentativa Ã© registrada em `retry_history`:

```json
{
  "retry_history": [
    {
      "attempt": 0,
      "engine": "BEAUTIFULSOUP",
      "use_proxy": false,
      "started_at": "2025-01-13T10:00:00Z",
      "completed_at": "2025-01-13T10:00:05Z",
      "status": "FAILED",
      "error_type": "TIMEOUT",
      "error_message": "Request timeout after 30s",
      "duration_seconds": 5
    },
    {
      "attempt": 1,
      "engine": "BEAUTIFULSOUP",
      "use_proxy": true,
      "started_at": "2025-01-13T10:00:10Z",
      "completed_at": "2025-01-13T10:00:12Z",
      "status": "FAILED",
      "error_type": "HTTP_ERROR",
      "error_message": "403 Forbidden",
      "duration_seconds": 2
    },
    {
      "attempt": 2,
      "engine": "PLAYWRIGHT",
      "use_proxy": false,
      "started_at": "2025-01-13T10:00:22Z",
      "completed_at": "2025-01-13T10:00:35Z",
      "status": "SUCCESS",
      "duration_seconds": 13
    }
  ],
  "retry_count": 2,
  "engine_used": "PLAYWRIGHT",
  "proxy_used": false,
  "total_duration_seconds": 20
}
```

---

#### 3.5.6 LÃ³gica de Retry (Worker)

**PseudocÃ³digo:**

```
def execute_crawler_with_retry(crawler_job, execution):
    retry_strategy = crawler_job.retry_strategy

    for attempt_config in retry_strategy:
        attempt_num = attempt_config['attempt']
        engine = attempt_config['engine']
        use_proxy = attempt_config['use_proxy']
        delay = attempt_config['delay_seconds']

        # Wait delay (backoff)
        if delay > 0:
            sleep(delay)

        # Update execution
        execution.current_retry_attempt = attempt_num

        try:
            # Select engine adapter
            if engine == "BEAUTIFULSOUP":
                adapter = BeautifulSoupCrawlerAdapter(use_proxy=use_proxy)
            else:
                adapter = PlaywrightCrawlerAdapter(use_proxy=use_proxy)

            # Execute crawl
            result = adapter.crawl(crawler_job.url)

            # SUCCESS
            execution.engine_used = engine
            execution.proxy_used = use_proxy
            execution.status = "COMPLETED"
            log_retry_attempt(execution, attempt_num, "SUCCESS")
            return result

        except Exception as e:
            # FAILED
            log_retry_attempt(execution, attempt_num, "FAILED", error=e)
            execution.retry_count += 1

            # Last attempt?
            if attempt_num >= crawler_job.max_retries:
                execution.status = "FAILED"
                execution.error_message = f"All retries exhausted: {e}"
                raise

            # Continue to next retry
            continue
```

---

#### 3.5.7 MÃ©tricas de Retry

**Analytics em Elasticsearch:**

```json
{
  "retry_metrics": {
    "total_executions": 1000,
    "executions_with_retries": 250,
    "retry_rate": 25.0,
    "success_by_attempt": {
      "0": 750,
      "1": 150,
      "2": 75,
      "3": 25
    },
    "success_by_engine": {
      "BEAUTIFULSOUP": 800,
      "PLAYWRIGHT": 200
    },
    "average_retries_per_execution": 0.5,
    "most_common_errors": [
      {"type": "TIMEOUT", "count": 120},
      {"type": "HTTP_403", "count": 80},
      {"type": "JAVASCRIPT_ERROR", "count": 50}
    ]
  }
}
```

**Queries Ãºteis:**

```
# Taxa de sucesso por tentativa
GET /crawler-executions-*/_search
{
  "aggs": {
    "by_attempt": {
      "terms": { "field": "current_retry_attempt" },
      "aggs": {
        "success_rate": {
          "filters": {
            "filters": {
              "success": { "term": { "status": "COMPLETED" } }
            }
          }
        }
      }
    }
  }
}

# Engine mais efetiva
GET /crawler-executions-*/_search
{
  "query": { "term": { "status": "COMPLETED" } },
  "aggs": {
    "by_engine": { "terms": { "field": "engine_used" } }
  }
}
```

---

#### 3.5.8 BenefÃ­cios do Sistema

âœ… **ResiliÃªncia:** NÃ£o falha imediatamente, tenta mÃºltiplas abordagens
âœ… **Economia:** ComeÃ§a com engine leve (BeautifulSoup) antes de usar Playwright
âœ… **Flexibilidade:** ConfigurÃ¡vel por crawler (cada site tem suas particularidades)
âœ… **Observabilidade:** HistÃ³rico completo de tentativas para anÃ¡lise
âœ… **Smart Fallback:** Aumenta "potÃªncia" progressivamente (bs4 â†’ bs4+proxy â†’ pw â†’ pw+proxy)
âœ… **Backoff Exponencial:** Delays crescentes evitam sobrecarga

---

## 4. Domain Layer (Entidades e ServiÃ§os)

### 4.1 Entities

#### CrawlerJob
**Agregado raiz: ConfiguraÃ§Ã£o de crawler**

**Responsabilidades:**
- Armazenar configuraÃ§Ã£o de crawl (URL, tipo, filtros, PDFs)
- Gerenciar agendamento (schedule_type, cron)
- Controlar estado (ativo, pausado, parado)
- Rastrear estatÃ­sticas (total_executions, success_rate)

**MÃ©todos principais:**
- `activate()` - Ativar crawler
- `pause()` - Pausar crawler (mantÃ©m config, nÃ£o executa)
- `stop()` - Parar permanentemente
- `update_schedule(cron)` - Atualizar agendamento
- `record_execution(success)` - Registrar execuÃ§Ã£o

---

#### CrawlerExecution
**Entidade: ExecuÃ§Ã£o individual de um crawler**

**Responsabilidades:**
- Rastrear progresso (0-100%)
- Contabilizar pÃ¡ginas/arquivos (downloaded, failed)
- Armazenar resultados (minio_folder_path)
- Registrar erros

**MÃ©todos principais:**
- `is_running()` - Verificar se estÃ¡ em execuÃ§Ã£o
- `is_completed()` - Verificar se finalizou
- `mark_failed(error)` - Marcar como falho
- `update_progress(percentage)` - Atualizar progresso

---

#### CrawledFile
**Entidade: Arquivo individual baixado**

**Responsabilidades:**
- Metadados do arquivo (URL, tipo, tamanho)
- Path no Min.io e URL pÃºblica
- Status (downloaded, failed, skipped)

---

### 4.2 Value Objects

#### URLPattern
**NormalizaÃ§Ã£o e detecÃ§Ã£o de duplicatas**

**PropÃ³sito:** Gerar padrÃ£o normalizado de URL para comparaÃ§Ã£o

**Exemplo:**
```
Input: https://Example.com/Page?id=123&sort=desc
Output (normalized): https://example.com/page?id=*&sort=*
Pattern: example.com/page?*
```

**Regras de normalizaÃ§Ã£o:**
- Lowercase domain
- Remove trailing slash
- Sort query parameters
- Substituir valores de params por wildcards (detecÃ§Ã£o de duplicatas)
- Remove fragment (#)

---

#### CrawlerSchedule
**ConfiguraÃ§Ã£o de agendamento**

**Responsabilidades:**
- Validar schedule_type (one_time, recurring)
- Validar cron expression
- Calcular next_run_at
- ConversÃ£o de timezone

---

#### DownloadConfig
**ConfiguraÃ§Ã£o de download**

**Responsabilidades:**
- Validar crawl_type
- Validar file_extensions
- Validar pdf_handling

---

### 4.3 Domain Services

#### URLNormalizerService
**NormalizaÃ§Ã£o de URLs**

**MÃ©todos:**
- `normalize_url(url)` - Normalizar para comparaÃ§Ã£o exata
- `generate_pattern(url)` - Gerar padrÃ£o para fuzzy matching

---

#### DuplicateDetectorService
**DetecÃ§Ã£o de crawlers duplicados**

**MÃ©todos:**
- `find_duplicates(url)` - Buscar crawlers com URL similar
- `has_duplicate(url)` - Verificar se existe duplicata

**LÃ³gica:**
1. Normalizar URL
2. Gerar padrÃ£o
3. Query Elasticsearch com fuzzy match em `url_pattern`
4. Retornar lista de crawlers similares

---

#### CrawlerProgressService
**CÃ¡lculo de progresso**

**MÃ©todos:**
- `calculate_progress(execution)` - Calcular progresso baseado em:
  - PÃ¡ginas processadas
  - Arquivos baixados
  - Etapa atual (crawling, downloading, merging, uploading)

**LÃ³gica:**
- Crawling: 0-20%
- Downloading: 20-80% (distribuÃ­do pelos arquivos)
- Merging PDFs: 80-90%
- Uploading Min.io: 90-100%

---

## 5. Application Layer (Use Cases)

### 5.1 CreateCrawlerJobUseCase
**Criar novo crawler agendado**

**Input:** CreateCrawlerJobDTO
- user_id, name, url
- crawl_type, file_extensions, pdf_handling
- schedule_type, cron_expression, timezone

**Fluxo:**
1. Validar URL (nÃ£o permitir IPs internos, localhost)
2. Normalizar URL e gerar padrÃ£o
3. Verificar duplicatas (DuplicateDetectorService)
4. Criar entidade CrawlerJob
5. Salvar no MySQL (CrawlerJobRepository)
6. Indexar no Elasticsearch
7. Se recurring: Registrar no Celery Beat
8. Retornar CrawlerJobDTO

**Output:** CrawlerJobDTO

---

### 5.2 ExecuteCrawlerJobUseCase
**Executar crawler manualmente (run now)**

**Input:** crawler_job_id

**Fluxo:**
1. Buscar CrawlerJob no repositÃ³rio
2. Validar se estÃ¡ ativo
3. Criar CrawlerExecution
4. Salvar no MySQL
5. Disparar task Celery (execute_crawler)
6. Atualizar execution com celery_task_id
7. Retornar CrawlerExecutionDTO

**Output:** CrawlerExecutionDTO

---

### 5.3 ListCrawlerJobsUseCase
**Listar crawlers do usuÃ¡rio**

**Input:** user_id, filters (status, type, search)

**Fluxo:**
1. Query no repositÃ³rio com filtros
2. Ordenar por created_at DESC
3. PaginaÃ§Ã£o (limit, offset)
4. Converter para DTOs

**Output:** List[CrawlerJobDTO]

---

### 5.4 UpdateCrawlerJobUseCase
**Atualizar configuraÃ§Ã£o de crawler**

**Input:** crawler_job_id, UpdateCrawlerJobDTO
- name, cron_expression, file_extensions, is_active

**Fluxo:**
1. Buscar CrawlerJob
2. Validar permissÃµes (user_id)
3. Atualizar campos
4. Atualizar no MySQL
5. Atualizar Ã­ndice Elasticsearch
6. Se mudou cron: Atualizar Celery Beat schedule
7. Retornar CrawlerJobDTO atualizado

**Output:** CrawlerJobDTO

---

### 5.5 GetCrawlerExecutionHistoryUseCase
**Obter histÃ³rico de execuÃ§Ãµes**

**Input:** crawler_job_id, filters (status, date_range)

**Fluxo:**
1. Buscar execuÃ§Ãµes no repositÃ³rio
2. Ordenar por started_at DESC
3. PaginaÃ§Ã£o
4. Enriquecer com estatÃ­sticas (success_rate, avg_duration)
5. Converter para DTOs

**Output:** List[CrawlerExecutionDTO]

---

### 5.6 PauseCrawlerJobUseCase
**Pausar crawler (nÃ£o executa, mas mantÃ©m config)**

**Input:** crawler_job_id

**Fluxo:**
1. Buscar CrawlerJob
2. Validar permissÃµes
3. `crawler_job.pause()` - altera is_active=False, status=PAUSED
4. Atualizar no MySQL
5. Remover do Celery Beat schedule (se recurring)
6. Cancelar execuÃ§Ãµes em andamento (opcional)

**Output:** CrawlerJobDTO

---

## 6. Infrastructure Layer

### 6.1 Repositories (MySQL)

#### MySQLCrawlerJobRepository
**ImplementaÃ§Ã£o do CrawlerJobRepository**

**MÃ©todos:**
- `save(crawler_job)` - Criar/atualizar
- `find_by_id(id)` - Buscar por ID
- `find_by_user_id(user_id)` - Buscar por usuÃ¡rio
- `find_by_url_pattern(pattern)` - Buscar por padrÃ£o de URL
- `find_active()` - Buscar ativos (para Celery Beat)
- `delete(id)` - Deletar (cascade para executions e files)

#### MySQLCrawlerExecutionRepository
**ImplementaÃ§Ã£o do CrawlerExecutionRepository**

**MÃ©todos:**
- `save(execution)` - Criar
- `update(execution)` - Atualizar (progresso, status)
- `find_by_id(id)` - Buscar por ID
- `find_by_crawler_job_id(job_id)` - HistÃ³rico de execuÃ§Ãµes
- `find_running()` - ExecuÃ§Ãµes em andamento

#### MySQLCrawledFileRepository
**ImplementaÃ§Ã£o do CrawledFileRepository**

**MÃ©todos:**
- `save(file)` - Registrar arquivo baixado
- `find_by_execution_id(execution_id)` - Arquivos de uma execuÃ§Ã£o
- `count_by_type(execution_id)` - Contar por tipo (pdf, xlsx, etc.)

---

### 6.2 Adapters

#### BeautifulSoupCrawlerAdapter
**ImplementaÃ§Ã£o do CrawlerPort (scraping)**

**Responsabilidades:**
- Fazer requests HTTP (httpx)
- Parse HTML (BeautifulSoup)
- Extrair links (com filtro por extensÃ£o)
- Download de arquivos binÃ¡rios
- Respeitar rate limits
- Respeitar robots.txt (opcional)

**MÃ©todos:**
- `crawl_page(url, file_extensions)` - Crawl pÃ¡gina e extrair links
- `download_file(url, destination)` - Download arquivo binÃ¡rio
- `close()` - Fechar cliente HTTP

---

#### PyPDFMergerAdapter
**ImplementaÃ§Ã£o do PDFMergerPort**

**Responsabilidades:**
- Merge de mÃºltiplos PDFs em um Ãºnico arquivo
- Adicionar bookmarks (TOC)
- Preservar metadados
- CompressÃ£o (opcional)

**MÃ©todos:**
- `merge_pdfs(pdf_files, output_path)` - Merge lista de PDFs
- `add_bookmarks(pdf, bookmarks)` - Adicionar TOC

---

#### MinioCrawlerStorageAdapter
**Storage de arquivos crawleados**

**Responsabilidades:**
- Upload de arquivos para Min.io
- Upload de pÃ¡ginas HTML
- Gerar URLs pÃºblicas
- Organizar estrutura de pastas

**MÃ©todos:**
- `upload_crawled_file(execution_id, filename, file_path)` - Upload arquivo
- `upload_html_page(execution_id, url, html_content)` - Upload HTML
- `get_execution_folder(execution_id)` - Path da pasta

---

### 6.3 Elasticsearch Storage

#### CrawlerJobIndex
**IndexaÃ§Ã£o de jobs**

**Responsabilidades:**
- Indexar novos jobs
- Atualizar jobs existentes
- Busca fuzzy por url_pattern
- Filtros e agregaÃ§Ãµes

**MÃ©todos:**
- `index_crawler_job(job)` - Indexar
- `update_crawler_job(job)` - Atualizar
- `search_by_url_pattern(pattern)` - Busca fuzzy
- `find_active_jobs()` - Jobs ativos

#### CrawlerExecutionIndex
**IndexaÃ§Ã£o de execuÃ§Ãµes (time-series)**

**Responsabilidades:**
- Indexar execuÃ§Ãµes completas
- Queries temporais
- AgregaÃ§Ãµes (sucesso/falha, duration)

#### CrawlerMetricsIndex
**MÃ©tricas tempo real**

**Responsabilidades:**
- Indexar mÃ©tricas durante execuÃ§Ã£o (bulk)
- Queries de progresso
- ILM para deleÃ§Ã£o automÃ¡tica

---

## 7. Workers Celery

### 7.1 Tasks

#### execute_crawler
**Task principal: Executar crawler**

**Responsabilidades:**
1. Buscar CrawlerJob e CrawlerExecution no MySQL
2. Instanciar CrawlerScraper
3. Executar scraping completo
4. Atualizar status e estatÃ­sticas
5. Indexar mÃ©tricas no Elasticsearch
6. Tratamento de erros e retry

**Fila:** `crawler` (isolada do resto do sistema)

---

#### schedule_crawler
**Task Beat: Agendar execuÃ§Ã£o (triggered por Celery Beat)**

**Responsabilidades:**
1. Buscar CrawlerJob
2. Verificar se estÃ¡ ativo
3. Criar nova CrawlerExecution
4. Disparar execute_crawler task
5. Atualizar next_run_at

**Fila:** `beat` (Celery Beat scheduler)

---

### 7.2 Worker Logic

#### CrawlerScraper
**OrquestraÃ§Ã£o do crawl**

**Fluxo de execuÃ§Ã£o:**
```
1. Crawl pÃ¡gina principal (BeautifulSoup)
   - Parse HTML
   - Extrair links
   - Salvar HTML no Min.io (opcional)

2. Filtrar links por extensÃ£o
   - file_extensions config
   - extension_categories

3. Download arquivos em paralelo
   - httpx async requests
   - Salvar em /tmp temporÃ¡rio
   - Registrar CrawledFile no MySQL

4. Processar PDFs (se pdf_handling != INDIVIDUAL)
   - Merge PDFs com PyPDF2
   - Adicionar bookmarks
   - Comprimir (opcional)

5. Upload para Min.io
   - Upload todos os arquivos
   - Estrutura: crawled/{execution_id}/
   - Gerar URLs pÃºblicas
   - Atualizar CrawledFile.public_url

6. Atualizar progresso (0-100%)
   - Atualizar MySQL
   - Indexar mÃ©tricas no Elasticsearch (bulk)

7. Cleanup
   - Deletar arquivos temporÃ¡rios
   - Marcar como COMPLETED
```

---

#### FileDownloader
**Download paralelo de arquivos**

**Responsabilidades:**
- Download assÃ­ncrono (httpx AsyncClient)
- Pool de workers (max_concurrent_downloads)
- Retry automÃ¡tico (3 tentativas)
- Progress tracking
- Error handling

---

#### PDFProcessor
**Processamento de PDFs**

**Responsabilidades:**
- Identificar PDFs baixados
- Merge (se pdf_handling == COMBINED ou BOTH)
- Validar PDFs (nÃ£o corrompidos)
- OtimizaÃ§Ã£o/compressÃ£o (opcional)

---

#### ProgressTracker
**Rastreamento de progresso em tempo real**

**Responsabilidades:**
- Calcular progresso baseado em etapas
- Atualizar MySQL (execution.progress)
- Indexar mÃ©tricas no Elasticsearch (bulk, a cada 5s)
- Publicar no Redis (para WebSocket real-time, opcional)

---

## 8. API Endpoints

### 8.1 Crawlers Management

| MÃ©todo | Endpoint | DescriÃ§Ã£o | Auth |
|--------|----------|-----------|------|
| POST | `/crawlers` | Criar novo crawler | JWT/API Key |
| GET | `/crawlers` | Listar crawlers do usuÃ¡rio | JWT/API Key |
| GET | `/crawlers/{id}` | Obter detalhes de um crawler | JWT/API Key |
| PATCH | `/crawlers/{id}` | Atualizar configuraÃ§Ã£o | JWT/API Key |
| DELETE | `/crawlers/{id}` | Deletar crawler | JWT/API Key |
| POST | `/crawlers/{id}/execute` | Executar manualmente (run now) | JWT/API Key |
| PATCH | `/crawlers/{id}/pause` | Pausar crawler | JWT/API Key |
| PATCH | `/crawlers/{id}/resume` | Retomar crawler pausado | JWT/API Key |

### 8.2 Executions & History

| MÃ©todo | Endpoint | DescriÃ§Ã£o | Auth |
|--------|----------|-----------|------|
| GET | `/crawlers/{id}/executions` | Listar execuÃ§Ãµes de um crawler | JWT/API Key |
| GET | `/crawlers/{id}/executions/{exec_id}` | Obter detalhes de uma execuÃ§Ã£o | JWT/API Key |
| GET | `/crawlers/{id}/executions/{exec_id}/files` | Listar arquivos baixados | JWT/API Key |
| GET | `/crawlers/{id}/executions/{exec_id}/progress` | Progresso em tempo real | JWT/API Key |
| POST | `/crawlers/{id}/executions/{exec_id}/cancel` | Cancelar execuÃ§Ã£o em andamento | JWT/API Key |

### 8.3 Analytics & Search

| MÃ©todo | Endpoint | DescriÃ§Ã£o | Auth |
|--------|----------|-----------|------|
| GET | `/crawlers/search` | Buscar crawlers por URL/padrÃ£o | JWT/API Key |
| GET | `/crawlers/stats` | EstatÃ­sticas gerais | JWT/API Key |
| GET | `/crawlers/{id}/stats` | EstatÃ­sticas de um crawler | JWT/API Key |

---

## 9. Fluxos de ExecuÃ§Ã£o

### 9.1 Criar Crawler Agendado

```
1. Frontend/User â†’ POST /crawlers
   Body: {
     name: "Crawl Example.com PDFs",
     url: "https://example.com/docs",
     crawl_type: "page_with_filtered",
     file_extensions: ["pdf"],
     pdf_handling: "both",
     schedule_type: "recurring",
     schedule_frequency: "daily",
     cron_expression: "0 9 * * *",  // Daily at 9 AM
     timezone: "America/Sao_Paulo"
   }

2. API â†’ CreateCrawlerJobUseCase.execute()
   â”œâ”€ Validar URL (nÃ£o localhost, nÃ£o IPs internos)
   â”œâ”€ Normalizar URL e gerar padrÃ£o
   â”œâ”€ DuplicateDetectorService.find_duplicates()
   â”‚   â””â”€ Query Elasticsearch (fuzzy match em url_pattern)
   â”œâ”€ Criar CrawlerJob entity
   â”œâ”€ CrawlerJobRepository.save() â†’ MySQL
   â”œâ”€ Elasticsearch.index_crawler_job()
   â”œâ”€ Se recurring:
   â”‚   â””â”€ Celery Beat: Registrar schedule
   â”‚       â””â”€ beat_schedule['crawler-{id}'] = {
   â”‚             'task': 'schedule_crawler',
   â”‚             'schedule': crontab(...),
   â”‚             'args': (crawler_job_id,)
   â”‚           }
   â””â”€ Retornar CrawlerJobResponse

3. Frontend â† 201 Created
   Body: {
     id: "crawler-123",
     name: "Crawl Example.com PDFs",
     status: "ACTIVE",
     next_run_at: "2025-01-14T09:00:00-03:00",
     ...
   }
```

---

### 9.2 ExecuÃ§Ã£o AutomÃ¡tica (Celery Beat)

```
1. Celery Beat â†’ Trigger schedule_crawler task
   (triggered by cron: 0 9 * * *)
   â†“
2. schedule_crawler(crawler_job_id)
   â”œâ”€ Buscar CrawlerJob no MySQL
   â”œâ”€ Verificar is_active == True
   â”œâ”€ Criar CrawlerExecution (status=PENDING)
   â”œâ”€ Salvar no MySQL
   â””â”€ Disparar execute_crawler.apply_async()
       â””â”€ Fila: 'crawler'
   â†“
3. Celery Worker pega execute_crawler task
   â†“
4. execute_crawler(crawler_job_id, execution_id)
   â”œâ”€ Buscar CrawlerJob e CrawlerExecution no MySQL
   â”œâ”€ Atualizar execution.celery_task_id
   â”œâ”€ Atualizar execution.status = PROCESSING
   â””â”€ CrawlerScraper(job, execution).run()
       â†“
       â”œâ”€ [10%] Crawl pÃ¡gina principal
       â”‚   â”œâ”€ httpx.get(url)
       â”‚   â”œâ”€ BeautifulSoup.parse()
       â”‚   â”œâ”€ Extrair links
       â”‚   â””â”€ Salvar HTML no Min.io (opcional)
       â†“
       â”œâ”€ [20%] Filtrar links por extensÃ£o
       â”‚   â””â”€ file_extensions: ["pdf"]
       â†“
       â”œâ”€ [30-80%] Download arquivos em paralelo
       â”‚   â”œâ”€ FileDownloader.download_all(urls)
       â”‚   â”œâ”€ Para cada arquivo:
       â”‚   â”‚   â”œâ”€ httpx.get(url) â†’ /tmp
       â”‚   â”‚   â”œâ”€ CrawledFileRepository.save()
       â”‚   â”‚   â””â”€ ProgressTracker.update(%)
       â”‚   â””â”€ Elasticsearch.bulk_index_metrics()
       â†“
       â”œâ”€ [80-90%] Processar PDFs (se pdf_handling != INDIVIDUAL)
       â”‚   â”œâ”€ PDFProcessor.merge_pdfs()
       â”‚   â””â”€ Salvar merged.pdf em /tmp
       â†“
       â”œâ”€ [90-100%] Upload para Min.io
       â”‚   â”œâ”€ Para cada arquivo em /tmp:
       â”‚   â”‚   â”œâ”€ MinioCrawlerStorageAdapter.upload()
       â”‚   â”‚   â”‚   â””â”€ Path: crawled/{execution_id}/files/{filename}
       â”‚   â”‚   â”œâ”€ Gerar public_url
       â”‚   â”‚   â””â”€ Atualizar CrawledFile.public_url
       â”‚   â””â”€ Atualizar execution.minio_folder_path
       â†“
       â””â”€ [100%] Finalizar
           â”œâ”€ execution.status = COMPLETED
           â”œâ”€ execution.completed_at = now()
           â”œâ”€ crawler_job.total_executions += 1
           â”œâ”€ crawler_job.successful_executions += 1
           â”œâ”€ crawler_job.last_execution_at = now()
           â”œâ”€ MySQL: commit()
           â”œâ”€ Elasticsearch: index_execution()
           â””â”€ Cleanup /tmp

5. Frontend pode consultar:
   GET /crawlers/{id}/executions/{execution_id}
   â†“
   Response: {
     id: "exec-456",
     status: "COMPLETED",
     progress: 100,
     files_downloaded: 10,
     minio_folder_path: "crawled/exec-456/",
     files: [
       {
         filename: "document1.pdf",
         public_url: "http://minio:9000/ingestify-crawled/crawled/exec-456/files/document1.pdf"
       },
       ...
     ]
   }
```

---

### 9.3 ExecuÃ§Ã£o Manual (Run Now)

```
1. Frontend â†’ POST /crawlers/{id}/execute
   â†“
2. ExecuteCrawlerJobUseCase.execute(crawler_job_id)
   â”œâ”€ Buscar CrawlerJob
   â”œâ”€ Validar is_active == True
   â”œâ”€ Criar CrawlerExecution
   â”œâ”€ Salvar no MySQL
   â””â”€ Disparar execute_crawler.apply_async()
   â†“
3. Fluxo idÃªntico ao 9.2 a partir do passo 3
```

---

### 9.4 Pausar Crawler

```
1. Frontend â†’ PATCH /crawlers/{id}/pause
   â†“
2. PauseCrawlerJobUseCase.execute(crawler_job_id)
   â”œâ”€ Buscar CrawlerJob
   â”œâ”€ crawler_job.pause()
   â”‚   â”œâ”€ is_active = False
   â”‚   â””â”€ status = PAUSED
   â”œâ”€ CrawlerJobRepository.update()
   â”œâ”€ Elasticsearch.update_crawler_job()
   â”œâ”€ Se recurring:
   â”‚   â””â”€ Celery Beat: Remover schedule
   â”‚       â””â”€ del beat_schedule['crawler-{id}']
   â””â”€ (Opcional) Cancelar execuÃ§Ãµes em andamento
       â””â”€ celery_app.control.revoke(task_id)
   â†“
3. Frontend â† 200 OK
   Body: {
     id: "crawler-123",
     status: "PAUSED",
     is_active: false,
     ...
   }
```

---

## 10. DetecÃ§Ã£o de Duplicatas

### 10.1 Funcionamento

**Objetivo:** Avisar usuÃ¡rio quando tenta criar crawler com URL similar a um existente.

**Fluxo:**
```
1. UsuÃ¡rio tenta criar crawler com URL: https://example.com/docs?page=1

2. URLNormalizerService.generate_pattern(url)
   â””â”€ Output: "example.com/docs?*"

3. DuplicateDetectorService.find_duplicates(url)
   â”œâ”€ Query Elasticsearch:
   â”‚   GET /crawler-jobs-*/_search
   â”‚   {
   â”‚     "query": {
   â”‚       "match": {
   â”‚         "url_pattern": {
   â”‚           "query": "example.com/docs?*",
   â”‚           "fuzziness": "AUTO"
   â”‚         }
   â”‚       }
   â”‚     }
   â”‚   }
   â””â”€ Retorna lista de crawlers similares

4. Se duplicatas encontradas:
   â””â”€ Retornar warning na resposta:
       {
         "id": "crawler-new",
         "warnings": [
           {
             "type": "duplicate_detected",
             "message": "Similar crawler already exists",
             "existing_crawlers": [
               {
                 "id": "crawler-123",
                 "name": "Existing Crawler",
                 "url": "https://example.com/docs?page=2",
                 "status": "ACTIVE"
               }
             ]
           }
         ]
       }

5. Frontend exibe aviso e opÃ§Ãµes:
   - "Ver crawler existente"
   - "Criar mesmo assim"
   - "Cancelar"
```

**Nota:** NÃ£o bloqueia criaÃ§Ã£o, apenas avisa.

---

## 11. Monitoramento e MÃ©tricas

### 11.1 MÃ©tricas em Tempo Real (via Elasticsearch)

**Queries Ãºteis:**

**1. Total de crawlers ativos:**
```
GET /crawler-jobs-*/_search
{
  "query": { "term": { "is_active": true } },
  "size": 0
}
```

**2. ExecuÃ§Ãµes por status (Ãºltimas 24h):**
```
GET /crawler-executions-*/_search
{
  "query": {
    "range": { "started_at": { "gte": "now-24h" } }
  },
  "aggs": {
    "by_status": { "terms": { "field": "status" } }
  }
}
```

**3. Taxa de sucesso (Ãºltimos 7 dias):**
```
GET /crawler-executions-*/_search
{
  "query": {
    "range": { "started_at": { "gte": "now-7d" } }
  },
  "aggs": {
    "success_rate": {
      "filters": {
        "filters": {
          "successful": { "term": { "status": "COMPLETED" } },
          "failed": { "term": { "status": "FAILED" } }
        }
      }
    }
  }
}
```

**4. Progresso de execuÃ§Ã£o em tempo real:**
```
GET /crawler-metrics-*/_search
{
  "query": {
    "term": { "execution_id": "exec-456" }
  },
  "sort": [{ "timestamp": "desc" }],
  "size": 1
}
```

---

### 11.2 Logs Estruturados

**Formato:** JSON com contexto

**Exemplo:**
```json
{
  "timestamp": "2025-01-14T09:15:23Z",
  "level": "INFO",
  "event": "crawler_execution_completed",
  "crawler_job_id": "crawler-123",
  "execution_id": "exec-456",
  "duration_seconds": 125,
  "files_downloaded": 10,
  "total_size_bytes": 5242880
}
```

---

## 12. SeguranÃ§a

### 12.1 ValidaÃ§Ãµes de URL

**Blacklist (nÃ£o permitir):**
- `localhost`, `127.0.0.1`, `0.0.0.0`
- IPs privados (10.x, 172.16.x, 192.168.x)
- IPs de metadados cloud (169.254.169.254)
- URLs com usuÃ¡rio:senha (http://user:pass@example.com)

**ValidaÃ§Ãµes:**
- Protocolo: apenas http/https
- DomÃ­nio: deve ser vÃ¡lido (DNS resolvÃ­vel)

---

### 12.2 Rate Limiting

**Por crawler:**
- `crawler_rate_limit_per_second`: 2 requests/s (default)
- Delay entre requests: 500ms

**Global:**
- Max concurrent crawlers: ilimitado (controlado por workers Celery)
- Max concurrent downloads per crawler: 5 (config)

---

### 12.3 AutenticaÃ§Ã£o e AutorizaÃ§Ã£o

**Todos os endpoints protegidos:**
- JWT Token (user sessions)
- API Key (programmatic access)

**Isolamento de dados:**
- UsuÃ¡rios sÃ³ veem seus prÃ³prios crawlers
- Filtro automÃ¡tico por `user_id` em queries

---

### 12.4 Storage Security

**Min.io:**
- Public read apenas para bucket `ingestify-crawled`
- Presigned URLs com expiraÃ§Ã£o (opcional para seguranÃ§a extra)
- Isolamento por `execution_id` (cada execuÃ§Ã£o em pasta separada)

---

## 13. ConfiguraÃ§Ã£o (backend/shared/config.py)

Adicionar ao `Settings`:

```python
# Crawler Configuration
crawler_enabled: bool = True  # Feature flag
crawler_max_concurrent_downloads: int = 5
crawler_max_concurrent_assets: int = 10  # Downloads de assets em paralelo
crawler_download_timeout_seconds: int = 60
crawler_user_agent: str = "IngestifyBot/1.0"
crawler_respect_robots_txt: bool = True
crawler_rate_limit_per_second: int = 2

# Crawler Engine Defaults
crawler_default_engine: str = "beautifulsoup"  # beautifulsoup ou playwright

# Playwright Configuration
playwright_headless: bool = True  # Rodar sem interface grÃ¡fica
playwright_timeout_seconds: int = 30  # Timeout para JS rendering
playwright_wait_for_selector: str = ""  # Opcional: esperar por elemento
playwright_browser_type: str = "chromium"  # chromium, firefox, webkit

# Proxy Configuration
proxy_enabled: bool = False  # Feature flag global
proxy_pool_enabled: bool = False  # Usar pool de proxies (rotaÃ§Ã£o)
proxy_rotation_strategy: str = "round_robin"  # round_robin, random, least_used

# Retry Configuration
crawler_retry_enabled: bool = True  # Feature flag para retries
crawler_max_retries: int = 3  # MÃ¡ximo de retries por execuÃ§Ã£o
crawler_retry_delay_base_seconds: int = 5  # Base para backoff exponencial
crawler_retry_strategy_default: str = "conservative"  # conservative, aggressive, proxy_first, balanced

# MinIO Buckets (novo)
minio_bucket_crawled: str = "ingestify-crawled"
```

---

## 14. DependÃªncias (requirements.txt)

Adicionar:

```txt
# Web Scraping - BeautifulSoup
beautifulsoup4>=4.12.0
httpx>=0.27.0
lxml>=5.0.0

# Web Scraping - Playwright (JavaScript rendering)
playwright>=1.40.0
# Instalar browsers: python -m playwright install chromium

# Proxy Support
httpx[socks]>=0.27.0  # SOCKS proxy support
python-socks>=2.4.0

# PDF Processing
PyPDF2>=3.0.0

# Cron Parsing
croniter>=2.0.0
```

**Notas:**
- Playwright requer instalaÃ§Ã£o de browsers: `python -m playwright install chromium`
- httpx[socks] adiciona suporte a proxies SOCKS5
- Playwright consome ~200MB de espaÃ§o (browser Chromium)

---

## 15. Database Migrations (Alembic)

**Comandos:**
```bash
# Gerar migration
alembic revision --autogenerate -m "Add crawler tables"

# Aplicar migration
alembic upgrade head
```

**Migration criarÃ¡:**
- Tabela `crawler_jobs`
- Tabela `crawler_executions`
- Tabela `crawled_files`
- Ãndices (user_id, url_pattern, status, created_at, etc.)
- Foreign keys e constraints

---

## 16. Cronograma de ImplementaÃ§Ã£o

### Sprint 1 (Semana 1-2): Foundation & Data Models
**Objetivo:** Estrutura de dados completa

- âœ… Modelos MySQL (crawler_jobs, crawler_executions, crawled_files)
- âœ… MigraÃ§Ãµes Alembic
- âœ… Elasticsearch indices (crawler-jobs-*, crawler-executions-*, crawler-metrics-*)
- âœ… Domain entities (CrawlerJob, CrawlerExecution, CrawledFile)
- âœ… Value Objects (URLPattern, CrawlerSchedule, DownloadConfig)
- âœ… Domain services (URLNormalizer, DuplicateDetector, ProgressCalculator)
- âœ… Testes unitÃ¡rios (domain layer)

**EntregÃ¡vel:** Models + migrations funcionando

---

### Sprint 2 (Semana 3-4): Infrastructure & Repositories
**Objetivo:** Camada de infraestrutura completa

- âœ… Repositories MySQL (CrawlerJob, CrawlerExecution, CrawledFile)
- âœ… Elasticsearch adapters (indexaÃ§Ã£o, busca, mÃ©tricas)
- âœ… BeautifulSoup crawler adapter (scraping + download)
- âœ… PyPDF merger adapter (merge de PDFs)
- âœ… Min.io crawler storage adapter (upload, public URLs)
- âœ… Bucket `ingestify-crawled` configurado
- âœ… Testes de integraÃ§Ã£o (repositories + adapters)

**EntregÃ¡vel:** Infrastructure layer testada

---

### Sprint 3 (Semana 5-6): Application Layer & Use Cases
**Objetivo:** LÃ³gica de negÃ³cio

- âœ… Use Cases:
  - CreateCrawlerJobUseCase
  - ExecuteCrawlerJobUseCase
  - ListCrawlerJobsUseCase
  - UpdateCrawlerJobUseCase
  - PauseCrawlerJobUseCase
  - GetCrawlerExecutionHistoryUseCase
- âœ… DTOs (CrawlerJobDTO, CrawlerExecutionDTO, CrawledFileDTO)
- âœ… Testes unitÃ¡rios (use cases)

**EntregÃ¡vel:** Use cases funcionando com mocks

---

### Sprint 4 (Semana 7-8): Workers Celery
**Objetivo:** ExecuÃ§Ã£o de crawls

- âœ… Celery tasks (execute_crawler, schedule_crawler)
- âœ… CrawlerScraper (orquestraÃ§Ã£o do crawl)
- âœ… FileDownloader (download paralelo)
- âœ… PDFProcessor (merge de PDFs)
- âœ… ProgressTracker (atualizaÃ§Ã£o tempo real)
- âœ… Celery Beat integration (agendamento recurring)
- âœ… Testes end-to-end (workers)

**EntregÃ¡vel:** Crawls executando com sucesso

---

### Sprint 5 (Semana 9-10): API & Presentation Layer
**Objetivo:** Endpoints REST

- âœ… CrawlerController (todos os endpoints)
- âœ… Request/Response schemas (Pydantic)
- âœ… Dependency injection (use cases)
- âœ… AutenticaÃ§Ã£o (JWT/API Key)
- âœ… ValidaÃ§Ãµes e error handling
- âœ… DocumentaÃ§Ã£o OpenAPI (Swagger)
- âœ… Testes de API (pytest + httpx)

**EntregÃ¡vel:** API completa e documentada

---

### Sprint 6 (Semana 11-12): Testing, Monitoring & Documentation
**Objetivo:** Qualidade e observabilidade

- âœ… Testes E2E (fluxos completos)
- âœ… MÃ©tricas Elasticsearch (queries, dashboards)
- âœ… Logs estruturados (structlog)
- âœ… Alertas (falhas, lentidÃ£o)
- âœ… DocumentaÃ§Ã£o completa:
  - README do mÃ³dulo crawler
  - Guia de uso (API docs)
  - Troubleshooting
- âœ… Performance tuning (Celery, Elasticsearch)

**EntregÃ¡vel:** Sistema pronto para produÃ§Ã£o

---

**Total:** 12 semanas (3 meses)

**Milestones:**
- Semana 2: Data models âœ…
- Semana 4: Infrastructure âœ…
- Semana 6: Business logic âœ…
- Semana 8: Workers funcionando âœ…
- Semana 10: API completa âœ…
- Semana 12: **Production ready** ğŸš€

---

## 17. Riscos e MitigaÃ§Ãµes

| Risco | Impacto | Probabilidade | MitigaÃ§Ã£o |
|-------|---------|---------------|-----------|
| Website bloqueia scraper | Alto | MÃ©dio | User-agent rotation, rate limiting, respeitar robots.txt |
| Crawls muito lentos | MÃ©dio | Baixo | Download paralelo, workers escalÃ¡veis, timeout configs |
| PDFs corrompidos no merge | MÃ©dio | Baixo | ValidaÃ§Ã£o antes de merge, try/catch, skip corrupted |
| Disk space (/tmp) cheio | Alto | Baixo | Cleanup after each execution, monitorar disk space |
| Elasticsearch lento | MÃ©dio | Baixo | Ãndices time-series, ILM, sharding adequado |
| Celery Beat nÃ£o dispara | Alto | Baixo | Health check task, logs, monitorar Beat |

---

## 18. MÃ©tricas de Sucesso

### 18.1 KPIs

- **Uptime:** 99.5% dos crawlers agendados executam no horÃ¡rio
- **Success Rate:** 95%+ das execuÃ§Ãµes completam sem erros
- **Performance:** MÃ©dia de 10 arquivos/minuto por crawler
- **Latency:** <2s para criaÃ§Ã£o de crawler via API
- **Storage:** <100MB por execuÃ§Ã£o (mÃ©dia)

### 18.2 Monitoramento

- Dashboard Elasticsearch (Kibana ou custom)
- Alertas para falhas recorrentes
- Logs centralizados (structlog + aggregator)
- Health checks periÃ³dicos

---

## 19. Melhorias Futuras (Post-MVP)

### Phase 2
- Dashboard visual React (frontend)
- WebSocket para progresso real-time
- Suporte a JavaScript rendering (Playwright)
- Webhooks para notificaÃ§Ãµes
- Export de dados (CSV, JSON, Excel)

### Phase 3
- Crawling distribuÃ­do (mÃºltiplos workers em paralelo)
- Change detection (diff entre execuÃ§Ãµes)
- IA para extraÃ§Ã£o de conteÃºdo (LLMs)
- IntegraÃ§Ã£o com Google Drive/Dropbox (upload de resultados)
- OCR para imagens (Tesseract)

---

## 20. ConclusÃ£o

### 20.1 Resumo

Este plano de integraÃ§Ã£o:

âœ… **Reutiliza 100% da infraestrutura existente**
âœ… **Segue Clean Architecture** (domain, application, infrastructure, presentation)
âœ… **MantÃ©m consistÃªncia** com sistema de Jobs existente
âœ… **EscalÃ¡vel** (workers Celery, Elasticsearch time-series)
âœ… **TestÃ¡vel** (unit, integration, e2e)
âœ… **ObservÃ¡vel** (mÃ©tricas Elasticsearch, logs estruturados)
âœ… **Seguro** (auth, rate limiting, validaÃ§Ãµes)
âœ… **FlexÃ­vel**:
  - **4 engines de crawling** (BeautifulSoup, BeautifulSoup+Proxy, Playwright, Playwright+Proxy)
  - **Download granular de assets** (HTML only ou por tipo: CSS, JS, images, fonts, videos)
  - **Suporte a proxies** (HTTP, HTTPS, SOCKS5) com autenticaÃ§Ã£o
  - **JavaScript rendering** (Playwright para SPAs e sites dinÃ¢micos)
âœ… **Resiliente**:
  - **Sistema de retry inteligente** com fallback progressivo de engines
  - **4 estratÃ©gias prÃ©-definidas** (conservative, aggressive, proxy_first, balanced)
  - **Backoff exponencial** com delays crescentes
  - **HistÃ³rico completo** de tentativas para anÃ¡lise e otimizaÃ§Ã£o

### 20.2 PrÃ³ximos Passos

1. âœ… **Aprovar este plano** (revisar e validar arquitetura)
2. ğŸ”² Criar branch `feature/crawler-integration`
3. ğŸ”² Implementar Sprint 1 (Foundation)
4. ğŸ”² Iterar com feedback e ajustes
5. ğŸ”² Deploy em produÃ§Ã£o apÃ³s Sprint 6

---

**Documento criado por:** Claude Code
**Data:** 2025-01-13
**Ãšltima atualizaÃ§Ã£o:** 2025-01-13
**Status:** âœ… Aguardando aprovaÃ§Ã£o
**VersÃ£o:** 1.2

---

## Changelog

### v1.2 (2025-01-13) - Sistema de Retry Inteligente
- âœ… **Sistema de retry com fallback de engines**
  - Retry progressivo: BS4 â†’ BS4+Proxy â†’ Playwright â†’ Playwright+Proxy
  - ConfigurÃ¡vel via `retry_strategy` JSON
  - Backoff exponencial com delays crescentes
- âœ… **4 estratÃ©gias prÃ©-definidas (templates)**
  - Conservative (BS4 first)
  - Aggressive (Playwright first)
  - Proxy First (sempre com proxy)
  - Balanced (mix de todas)
- âœ… **Rastreamento completo de retries**
  - Campo `retry_history` com histÃ³rico JSON
  - Campos `retry_count`, `current_retry_attempt`, `engine_used`, `proxy_used`
  - MÃ©tricas de retry em Elasticsearch
- âœ… **ConfiguraÃ§Ãµes de retry**
  - `crawler_retry_enabled`, `crawler_max_retries`, `crawler_retry_strategy_default`
- âœ… **DocumentaÃ§Ã£o completa**
  - SeÃ§Ã£o 3.5 - Sistema de Retry com Fallback
  - PseudocÃ³digo de implementaÃ§Ã£o
  - Queries de analytics

### v1.1 (2025-01-13) - Engines e Assets FlexÃ­veis
- âœ… Adicionado suporte a mÃºltiplas engines (BeautifulSoup/Playwright)
- âœ… Adicionado suporte a proxies (HTTP, HTTPS, SOCKS5)
- âœ… Adicionado download granular de assets (CSS, JS, images, fonts, videos)
- âœ… Adicionado modo "HTML only" (sem assets)
- âœ… Atualizado modelo de dados com novos campos
- âœ… Atualizado dependÃªncias (playwright, httpx[socks], python-socks)
- âœ… Atualizado configuraÃ§Ã£o com settings de Playwright e proxies

### v1.0 (2025-01-13) - Plano Inicial
- âœ… Arquitetura Clean Architecture completa
- âœ… Modelos de dados (MySQL + Elasticsearch + Min.io)
- âœ… Domain Layer, Application Layer, Infrastructure Layer
- âœ… Workers Celery com agendamento
- âœ… API REST endpoints
- âœ… Cronograma de 12 semanas (6 sprints)
